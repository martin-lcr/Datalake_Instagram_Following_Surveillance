from airflow import DAG
from airflow.decorators import task
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta
import pendulum
import os
import re
import subprocess
import glob

# -------------------------------------------------------------------------
# Configuration et constantes
# -------------------------------------------------------------------------

# RÃ©pertoire de travail Airflow (oÃ¹ sont montÃ©s les volumes Docker)
BASE_DIR = "/opt/airflow"
SCRIPTS_DIR = os.path.join(BASE_DIR, "scripts")
ACCOUNTS_FILE = os.path.join(BASE_DIR, "instagram_accounts_to_scrape.txt")  # Fichier Ã  la racine
UNIFIED_SCRIPT = os.path.join(SCRIPTS_DIR, "instagram_scraping_ml_pipeline.py")
JARS_PATH = "/opt/airflow/jars/postgresql-42.2.27.jar"  # Jar PostgreSQL
ES_SPARK_JAR = "/opt/airflow/jars/elasticsearch-spark-30_2.12-8.11.0.jar"  # Jar ES (Scala 2.12)
DATA_DIR = os.path.join(BASE_DIR, "data")  # RÃ©pertoire de donnÃ©es

# Configuration base de donnÃ©es (Docker-compatible)
POSTGRES_HOST = os.getenv("POSTGRES_HOST", "postgres")  # Nom du service Docker
POSTGRES_PORT = os.getenv("POSTGRES_PORT", "5432")
POSTGRES_DB = os.getenv("POSTGRES_DB", "airflow")
POSTGRES_USER = os.getenv("POSTGRES_USER", "airflow")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "airflow")

# Configuration Elasticsearch (Docker-compatible)
ELASTICSEARCH_HOST = os.getenv("ELASTICSEARCH_HOST", "elasticsearch")
ELASTICSEARCH_PORT = os.getenv("ELASTICSEARCH_PORT", "9200")

# Exemple d'ancien snapshot pour comparaison globale (optionnel)
OLD_PARQUET_PATH = os.path.join(
    DATA_DIR,
    "usage_to_combined",
    "scraping",
    "instagram_data",
    "20250228",
    "1243",
    "final_aggregated.parquet"
)

default_args = {
    'owner': 'instagram_surveillance',
    'start_date': days_ago(1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Note: Le timezone Europe/Paris est configurÃ© via AIRFLOW__CORE__DEFAULT_TIMEZONE dans docker-compose.yml
dag = DAG(
    'instagram_scraping_surveillance_pipeline',
    default_args=default_args,
    description='Pipeline de surveillance Instagram ~4h (6x/jour) avec 3 passes + dÃ©lais alÃ©atoires + agrÃ©gation Ã  23h',
    schedule_interval='0 2,6,10,14,18,23 * * *',  # DÃ©clenchement Ã  2h, 6h, 10h, 14h, 18h, 23h puis dÃ©lai alÃ©atoire pour variation quotidienne
    catchup=False,
    tags=['instagram', 'scraping', 'ml', 'surveillance']
)

# -------------------------------------------------------------------------
# TÃ¢che 1 : LECTURE DES COMPTES Ã€ SCRAPER (TaskFlow)
# -------------------------------------------------------------------------
@task
def read_accounts():
    """
    Lit le fichier ACCOUNTS_FILE et retourne une liste de dictionnaires
    contenant les informations de compte pour chaque username Ã  scraper.
    """
    # VÃ©rifier que le script unifiÃ© existe
    if not os.path.exists(UNIFIED_SCRIPT):
        raise FileNotFoundError(f"Script unifiÃ© introuvable : {UNIFIED_SCRIPT}")

    # Lire les comptes Ã  scraper
    if not os.path.exists(ACCOUNTS_FILE):
        raise FileNotFoundError(f"Fichier des comptes introuvable : {ACCOUNTS_FILE}")

    with open(ACCOUNTS_FILE, "r", encoding="utf-8") as f:
        accounts = [line.strip() for line in f if line.strip() and not line.startswith('#')]

    if not accounts:
        raise ValueError(f"Aucun compte trouvÃ© dans {ACCOUNTS_FILE}")

    print(f"ğŸ“‹ [read_accounts] Comptes dÃ©tectÃ©s : {accounts}")

    account_list = []
    for account in accounts:
        # Normalisation du nom de compte (remplacer . et _ par -)
        normalized = account.replace(".", "-").replace("_", "-")

        account_list.append({
            "account": account,
            "normalized": normalized
        })

        print(f"âœ… [read_accounts] Compte ajoutÃ© : @{account} (normalisÃ©: {normalized})")

    print(f"âœ… [read_accounts] {len(account_list)} comptes Ã  scraper")
    return account_list

# -------------------------------------------------------------------------
# TÃ¢che 2 : EXÃ‰CUTION DU SCRAPING EN PARALLÃˆLE (TaskFlow avec Mapping)
# -------------------------------------------------------------------------
@task(max_active_tis_per_dag=1)  # Une seule tÃ¢che Ã  la fois pour Ã©viter conflits Selenium
def run_single_account_scraping(account_info: dict):
    """
    ExÃ©cute le script unifiÃ© de scraping pour un compte Instagram.
    Le script intÃ¨gre : scraping multi-passes + ML + stockage multi-couches.

    âš ï¸ DÃ©lai alÃ©atoire de 0-45min ajoutÃ© au dÃ©marrage pour Ã©viter la dÃ©tection Instagram
    """
    import sys
    import time
    import random

    account = account_info['account']

    # DÃ©lai alÃ©atoire de 0 Ã  45 minutes pour Ã©viter dÃ©tection Instagram
    random_delay_seconds = random.randint(0, 45 * 60)  # 0 Ã  2700 secondes (45 minutes)
    random_delay_minutes = random_delay_seconds // 60
    random_delay_remaining = random_delay_seconds % 60

    print(f"â° [Anti-dÃ©tection] DÃ©lai alÃ©atoire appliquÃ© : {random_delay_minutes}min {random_delay_remaining}s")
    print(f"â° [Anti-dÃ©tection] DÃ©marrage rÃ©el prÃ©vu Ã  : {datetime.now() + timedelta(seconds=random_delay_seconds)}")
    time.sleep(random_delay_seconds)

    print(f"ğŸŒ€ [run_single_account_scraping] DÃ©marrage scraping pour @{account}...")

    # Commande pour exÃ©cuter le script unifiÃ© avec l'account en paramÃ¨tre
    command = f"{sys.executable} {UNIFIED_SCRIPT} {account}"
    print(f"ğŸŒ€ [run_single_account_scraping] Commande : {command}")
    print(f"ğŸ [run_single_account_scraping] Python: {sys.executable} (version {sys.version.split()[0]})")

    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            timeout=7200  # Timeout 2 heures (scraping peut Ãªtre long)
        )

        print(f"ğŸ“Š [run_single_account_scraping] Sortie pour @{account} :")
        print(result.stdout)

        if result.returncode != 0:
            print(f"âŒ [run_single_account_scraping] Erreur pour @{account}")
            print(f"âŒ Stderr : {result.stderr}")
            raise Exception(f"Scraping failed for {account} with return code {result.returncode}")

        print(f"âœ… [run_single_account_scraping] Scraping terminÃ© avec succÃ¨s pour @{account}")

    except subprocess.TimeoutExpired:
        print(f"âŒ [run_single_account_scraping] Timeout dÃ©passÃ© pour @{account}")
        raise
    except Exception as e:
        print(f"âŒ [run_single_account_scraping] Erreur pour @{account} : {e}")
        raise

    return account_info

# -------------------------------------------------------------------------
# TÃ¢che 3 : AGRÃ‰GATION DES RÃ‰SULTATS + CHARGEMENT EN BDD (PythonOperator)
# -------------------------------------------------------------------------
def aggregate_results(**kwargs):
    """
    1) Lit pour chaque compte :
         - "formatted_parquet_with_ML.parquet" (tables finales)
         - "comparatif_parquet_with_ML.parquet" (tables comparatives) si elles existent
    2) AgrÃ¨ge et Ã©crit deux Parquets dans usage_to_combined :
         - final_aggregated.parquet
         - final_comparatif.parquet
    3) Compare le final agrÃ©gÃ© avec OLD_PARQUET_PATH si prÃ©sent.
    4) InsÃ¨re ces Parquets dans PostgreSQL.
    5) Push les chemins des Parquets via XCom pour la tÃ¢che d'indexation.

    âš ï¸ Cette tÃ¢che ne s'exÃ©cute qu'Ã  23h00 (agrÃ©gation quotidienne uniquement)
    """
    import glob
    import os
    from datetime import datetime
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import lit

    # VÃ©rification horaire : utiliser l'heure de dÃ©clenchement PRÃ‰VUE, pas l'heure actuelle
    # (important car le scraping peut avoir un dÃ©lai alÃ©atoire de 0-45min)
    ti = kwargs['ti']
    execution_date = kwargs['execution_date']
    scheduled_hour = execution_date.hour

    print(f"â° [aggregate_results] Heure de dÃ©clenchement prÃ©vue : {scheduled_hour}h")
    print(f"â° [aggregate_results] Heure actuelle : {datetime.now().strftime('%H:%M:%S')}")

    if scheduled_hour != 23:
        print(f"â­ï¸ [aggregate_results] AgrÃ©gation uniquement pour l'exÃ©cution de 23h00")
        print(f"â­ï¸ [aggregate_results] TÃ¢che skippÃ©e")
        return

    spark = SparkSession.builder \
        .appName("Aggregation_Scraping") \
        .config("spark.jars", JARS_PATH) \
        .getOrCreate()

    ti = kwargs['ti']
    account_list = ti.xcom_pull(key="return_value", task_ids="read_accounts")
    if not account_list:
        print("ğŸ’¡ [aggregate_results] Aucune donnÃ©e Ã  agrÃ©ger (pas de comptes).")
        spark.stop()
        return

    aggregated_final_df = None
    aggregated_comp_df = None
    current_date = datetime.now().strftime("%Y%m%d")
    current_time = datetime.now().strftime("%H%M")

    # Parcours des comptes pour lire "final" et "comparatif"
    for info in account_list:
        norm = info["normalized"]
        account = info["account"]

        print(f"ğŸ“‚ [aggregate_results] Traitement du compte @{account} (normalisÃ©: {norm})")

        usage_final_pattern = os.path.join(
            DATA_DIR, "usage", "scraping",
            f"instagram_data_{norm}",
            current_date,
            "*",
            "formatted_parquet_with_ML.parquet"
        )
        final_files = glob.glob(usage_final_pattern)

        if final_files:
            for fpath in final_files:
                print(f"ğŸ” [aggregate_results] Lecture FINAL {fpath} pour {norm}")
                try:
                    df_final = spark.read.parquet(fpath)
                    df_final = df_final.withColumn("username_scraped", lit(account))
                    aggregated_final_df = df_final if aggregated_final_df is None else aggregated_final_df.unionByName(df_final, allowMissingColumns=True)
                except Exception as e:
                    print(f"âŒ [aggregate_results] Erreur lecture finale {fpath} : {e}")
        else:
            print(f"ğŸ’¡ [aggregate_results] Aucune table finale trouvÃ©e pour {norm} avec pattern {usage_final_pattern}")

        usage_comp_pattern = os.path.join(
            DATA_DIR, "usage", "scraping",
            f"instagram_data_{norm}",
            current_date,
            "*",
            "comparatif_parquet_with_ML.parquet"
        )
        comp_files = glob.glob(usage_comp_pattern)

        if comp_files:
            for fpath in comp_files:
                print(f"ğŸ” [aggregate_results] Lecture COMPARATIF {fpath} pour {norm}")
                try:
                    df_comp = spark.read.parquet(fpath)
                    df_comp = df_comp.withColumn("username_scraped", lit(account))
                    aggregated_comp_df = df_comp if aggregated_comp_df is None else aggregated_comp_df.unionByName(df_comp, allowMissingColumns=True)
                except Exception as e:
                    print(f"âŒ [aggregate_results] Erreur lecture comparatif {fpath} : {e}")
        else:
            print(f"ğŸ’¡ [aggregate_results] Aucune table comparatif trouvÃ©e pour {norm}")

    # Ã‰criture des fichiers agrÃ©gÃ©s
    combined_path = os.path.join(
        DATA_DIR, "usage_to_combined", "scraping",
        "instagram_data", current_date, current_time
    )
    os.makedirs(combined_path, exist_ok=True)
    final_aggregated_file = os.path.join(combined_path, "final_aggregated.parquet")
    final_comp_file = os.path.join(combined_path, "final_comparatif.parquet")

    if aggregated_final_df is not None:
        aggregated_final_df.write.mode("append").parquet(final_aggregated_file)
        print(f"âœ… [aggregate_results] final_aggregated.parquet => {final_aggregated_file}")
        print(f"ğŸ“Š [aggregate_results] Nombre total de lignes : {aggregated_final_df.count()}")
        aggregated_final_df.show(10, truncate=False)
    else:
        print("ğŸ’¡ [aggregate_results] Aucun DF final agrÃ©gÃ© Ã  Ã©crire.")

    if aggregated_comp_df is not None:
        aggregated_comp_df.write.mode("append").parquet(final_comp_file)
        print(f"âœ… [aggregate_results] final_comparatif.parquet => {final_comp_file}")
        print(f"ğŸ“Š [aggregate_results] Nombre total de changements : {aggregated_comp_df.count()}")
        aggregated_comp_df.show(10, truncate=False)
    else:
        print("ğŸ’¡ [aggregate_results] Aucun DF comparatif agrÃ©gÃ© Ã  Ã©crire.")

    # Comparaison globale (optionnel)
    if aggregated_final_df is not None and os.path.exists(OLD_PARQUET_PATH):
        print("ğŸ“Œ [aggregate_results] Ancien snapshot dÃ©tectÃ©, comparaison globale en cours ...")
        try:
            oldDF = spark.read.parquet(OLD_PARQUET_PATH)
            join_cols = ["username", "full_name"]
            added = aggregated_final_df.join(oldDF, join_cols, "left_anti").withColumn("change", lit("added_global"))
            deleted = oldDF.join(aggregated_final_df, join_cols, "left_anti").withColumn("change", lit("deleted_global"))
            global_compDF = added.unionByName(deleted, allowMissingColumns=True)
            global_comp_file = os.path.join(combined_path, "final_global_comparatif.parquet")
            global_compDF.write.mode("append").parquet(global_comp_file)
            print(f"âœ… [aggregate_results] final_global_comparatif.parquet => {global_comp_file}")
            global_compDF.show(20, truncate=False)
        except Exception as e:
            print(f"âŒ [aggregate_results] Erreur comparaison globale : {e}")
    else:
        print("ğŸ›‘ [aggregate_results] Pas d'ancien snapshot ou pas de DF final, pas de comparaison globale.")

    # Insertion en PostgreSQL
    postgres_url = f"jdbc:postgresql://{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"

    if aggregated_final_df is not None:
        try:
            print(f"ğŸ”µ [aggregate_results] Insertion du Parquet final_aggregated dans PostgreSQL ({postgres_url}) ...")
            aggregated_final_df.write \
                .format("jdbc") \
                .option("url", postgres_url) \
                .option("dbtable", "final_aggregated_scraping") \
                .option("user", POSTGRES_USER) \
                .option("password", POSTGRES_PASSWORD) \
                .option("driver", "org.postgresql.Driver") \
                .mode("append") \
                .save()
            print("âœ… [aggregate_results] final_aggregated.parquet insÃ©rÃ© dans 'final_aggregated_scraping'.")
        except Exception as e:
            print(f"âŒ [aggregate_results] Erreur lors de l'insertion final_aggregated : {e}")

    if aggregated_comp_df is not None:
        try:
            print(f"ğŸ”µ [aggregate_results] Insertion du Parquet final_comparatif dans PostgreSQL ({postgres_url}) ...")
            aggregated_comp_df.write \
                .format("jdbc") \
                .option("url", postgres_url) \
                .option("dbtable", "final_comparatif_scraping") \
                .option("user", POSTGRES_USER) \
                .option("password", POSTGRES_PASSWORD) \
                .option("driver", "org.postgresql.Driver") \
                .mode("append") \
                .save()
            print("âœ… [aggregate_results] final_comparatif.parquet insÃ©rÃ© dans 'final_comparatif_scraping'.")
        except Exception as e:
            print(f"âŒ [aggregate_results] Erreur lors de l'insertion final_comparatif : {e}")

    # Push des chemins pour la tÃ¢che 4
    ti.xcom_push(key="final_aggregated_file", value=final_aggregated_file)
    ti.xcom_push(key="final_comparatif_file", value=final_comp_file)

    spark.stop()

aggregate_task = PythonOperator(
    task_id="aggregate_results",
    python_callable=aggregate_results,
    provide_context=True,
    dag=dag
)

# -------------------------------------------------------------------------
# TÃ¢che 4 : INDEXATION ELASTICSEARCH (PythonOperator)
# -------------------------------------------------------------------------
def index_to_elasticsearch(**kwargs):
    """
    Lit final_aggregated.parquet et final_comparatif.parquet (via XCom)
    et les envoie dans Elasticsearch.

    âš ï¸ Cette tÃ¢che ne s'exÃ©cute qu'Ã  23h00 (indexation quotidienne uniquement)
    """
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import lit
    from datetime import datetime

    # VÃ©rification horaire : utiliser l'heure de dÃ©clenchement PRÃ‰VUE, pas l'heure actuelle
    # (important car le scraping peut avoir un dÃ©lai alÃ©atoire de 0-45min)
    execution_date = kwargs['execution_date']
    scheduled_hour = execution_date.hour

    print(f"â° [index_to_elasticsearch] Heure de dÃ©clenchement prÃ©vue : {scheduled_hour}h")
    print(f"â° [index_to_elasticsearch] Heure actuelle : {datetime.now().strftime('%H:%M:%S')}")

    if scheduled_hour != 23:
        print(f"â­ï¸ [index_to_elasticsearch] Indexation uniquement pour l'exÃ©cution de 23h00")
        print(f"â­ï¸ [index_to_elasticsearch] TÃ¢che skippÃ©e")
        return

    ti = kwargs['ti']
    final_aggregated_file = ti.xcom_pull(task_ids="aggregate_results", key="final_aggregated_file")
    final_comparatif_file = ti.xcom_pull(task_ids="aggregate_results", key="final_comparatif_file")

    if not final_aggregated_file:
        print("ğŸ’¡ [index_to_elasticsearch] Pas de chemin parquet reÃ§u, indexation annulÃ©e.")
        return

    # ConcatÃ©nation des jars PostgreSQL et Elasticsearch-Spark
    jars_list = f"{JARS_PATH},{ES_SPARK_JAR}"
    spark = SparkSession.builder \
        .appName("IndexationElasticsearch_Scraping") \
        .config("spark.jars", jars_list) \
        .getOrCreate()

    # Indexation final_aggregated
    if final_aggregated_file and os.path.exists(final_aggregated_file):
        try:
            df_final = spark.read.parquet(final_aggregated_file)
            df_final = df_final.withColumn("indexed_at", lit(datetime.now().strftime("%Y-%m-%d %H:%M:%S")))

            df_final.write \
                .format("org.elasticsearch.spark.sql") \
                .option("es.nodes", ELASTICSEARCH_HOST) \
                .option("es.port", ELASTICSEARCH_PORT) \
                .option("es.nodes.wan.only", "true") \
                .option("es.resource", "instagram_scraping_aggregated") \
                .option("es.mapping.id", "username") \
                .mode("append") \
                .save()
            print(f"âœ… [Elasticsearch] final_aggregated.parquet indexÃ© dans 'instagram_scraping_aggregated' ({ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}).")
        except Exception as e:
            print(f"âŒ [Elasticsearch] Erreur indexation final_aggregated : {e}")

    # Indexation final_comparatif
    if final_comparatif_file and os.path.exists(final_comparatif_file):
        try:
            df_comp = spark.read.parquet(final_comparatif_file)
            df_comp = df_comp.withColumn("indexed_at", lit(datetime.now().strftime("%Y-%m-%d %H:%M:%S")))

            df_comp.write \
                .format("org.elasticsearch.spark.sql") \
                .option("es.nodes", ELASTICSEARCH_HOST) \
                .option("es.port", ELASTICSEARCH_PORT) \
                .option("es.nodes.wan.only", "true") \
                .option("es.resource", "instagram_scraping_comparatif") \
                .option("es.mapping.id", "username") \
                .mode("append") \
                .save()
            print(f"âœ… [Elasticsearch] final_comparatif.parquet indexÃ© dans 'instagram_scraping_comparatif' ({ELASTICSEARCH_HOST}:{ELASTICSEARCH_PORT}).")
        except Exception as e:
            print(f"âŒ [Elasticsearch] Erreur indexation final_comparatif : {e}")

    spark.stop()

index_task = PythonOperator(
    task_id="index_to_elasticsearch",
    python_callable=index_to_elasticsearch,
    provide_context=True,
    dag=dag
)

# -------------------------------------------------------------------------
# ORDONNANCEMENT
# -------------------------------------------------------------------------
with dag:
    accounts = read_accounts()
    run_all_accounts = run_single_account_scraping.expand(account_info=accounts)
    accounts >> run_all_accounts >> aggregate_task >> index_task
